# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A0sbvTmIItF_bme3awNZYAuMb7lO6Ukx
"""

import numpy as np
import json
import pandas as pd
import re
import string
import pickle
import random

from sklearn.preprocessing import LabelEncoder

from gensim.models import Word2Vec

from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer


from keras.layers import Embedding,LSTM,Dense,Input
from keras.models import Sequential
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import Adam
from keras.models import Model
from keras.layers import Dropout

"""Loading JSON Dataset"""

intents = json.loads(open(r'D:\\vs code\\python\\DeepLearning\\Projects\\Karen\\v2\\Datasets\\dataset2.json').read())

lemmatizer = WordNetLemmatizer()

def label(sentence):
    functional = ["battery_level","stackoverflow","youtube","google","time","date","chatbot_goodbye","cpu_temperature","activate_gesture_recognition","deactivate_gesture_recognition"]

    if sentence[0] in functional:
        sentence[0] = "functional"
    else:
        sentence[0] = "non_functional"

    return sentence

data = []

for intent in intents['intents']:
    for pattern in intent['patterns']:
        c = [intent['tag'],pattern]
        data.append(c)

data_new = []

for sent in data:
    sent1 = label(sent)
    data_new.append(sent1)

random.shuffle(data_new)

"""Converting the JSON Dataset into pandas Dataset"""

df = pd.DataFrame(data=data_new,columns=['tags','patterns'])

df.to_csv(r'D:\\vs code\\python\\DeepLearning\\Projects\\Karen\\v2\\Datasets\\chatbot_datasetv2.csv',index=False)

"""Loading the Dataset"""

dataset = pd.read_csv(r'D:\\vs code\\python\\DeepLearning\\Projects\\Karen\\v2\\Datasets\\chatbot_datasetv2.csv')

dataset.head()

"""Data Cleaning"""

def preprocessing(sentence):
    sentence = sentence.lower()
    sentence = re.sub(r"i'm", "i am", sentence)
    sentence = re.sub(r"he's", "he is", sentence)
    sentence = re.sub(r"she's", "she is", sentence)
    sentence = re.sub(r"it's", "it is", sentence)
    sentence = re.sub(r"that's", "that is", sentence)
    sentence = re.sub(r"what's", "that is", sentence)
    sentence = re.sub(r"where's", "where is", sentence)
    sentence = re.sub(r"how's", "how is", sentence)
    sentence = re.sub(r"\'ll", " will", sentence)
    sentence = re.sub(r"\'ve", " have", sentence)
    sentence = re.sub(r"\'re", " are", sentence)
    sentence = re.sub(r"\'d", " would", sentence)
    sentence = re.sub(r"\'re", " are", sentence)
    sentence = re.sub(r"won't", "will not", sentence)
    sentence = re.sub(r"can't", "cannot", sentence)
    sentence = re.sub(r"n't", " not", sentence)
    sentence = re.sub(r"'til", "until", sentence)
    sentence = re.sub(r"who's", "who is", sentence)
    sentence = re.sub(r"[-()\"#/@;:<>{}`+=~|.\,?]", "", sentence)
    sentence = sentence.split()
    sentence = [lemmatizer.lemmatize(word) for word in sentence]
    sentence = [lemmatizer.lemmatize(word,pos='v') for word in sentence]
    sentence = ' '.join(sentence)

    return sentence

"""Applying Preprocessing Function"""

dataset['patterns'] = dataset['patterns'].apply(lambda x: preprocessing(x))

df.to_csv(r'D:\\vs code\\python\\DeepLearning\\Projects\\Karen\\v2\Datasets\\chatbot_dataset_cleanedv2.csv',index=False)

encoder = LabelEncoder()

encoder_y = encoder.fit_transform(dataset['tags'])
x = dataset['patterns'].values
y = encoder_y.reshape(-1,1)

for i,j in zip(y,dataset['tags']):
    print(i,j)

"""Creating a Corpus"""

a = set()

for sentence in x:
    sentence = sentence.split()
    for word in sentence:
        a.add(word)

len(a) # corpus size

# finding the maximum sentence length

maxlen = 0

for sentence in x:
    sentence = sentence.split()
    if len(sentence) > maxlen:
        maxlen = len(sentence)

print(maxlen)

# type conversion
x = x.tolist()

"""Tokenisation"""

tokenizer = Tokenizer(num_words=500)
tokenizer.fit_on_texts(x)

vocab_size = len(tokenizer.word_index) + 1

x_sequences = tokenizer.texts_to_sequences(x)

padding_length = maxlen + 5

padded_sentences = pad_sequences(x_sequences,maxlen=padding_length)

with open(r"D:\\vs code\\python\\DeepLearning\\Projects\\Karen\\v2\\Pickles\\padding_lengthv2.pickle",'wb') as file:
    pickle.dump(padding_length,file)
    file.close()

vector_length = 100

# Creating a Glove Dictionary

file_path = f"D:\\Datasets\\glove\\glove.6B.{vector_length}d.txt"

embeddings_dictionary = dict()

file = open(file_path,encoding='utf8')

for lines in file:
    records = lines.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary[word] = vector_dimensions

file.close()

embedding_matrix = np.zeros((vocab_size,vector_length))

for word, index in tokenizer.word_index.items():
    vector = embeddings_dictionary.get(word)
    if vector is not None:
        embedding_matrix[index] = vector

with open(r'D:\\vs code\\python\\DeepLearning\\Projects\\Karen\\v2\\Pickles\\my_dictv2.pickle', 'wb') as file:
    pickle.dump(embeddings_dictionary, file)
    file.close()

with open(r'D:\\vs code\\python\\DeepLearning\\Projects\\Karen\\v2\\Pickles\\my_tokenizer.picklev2','wb') as file:
    pickle.dump(tokenizer,file)
    file.close()

"""The Brain"""

deep_input = Input(shape=(maxlen + 5,))

embedding_layer = Embedding(vocab_size,vector_length,weights=[embedding_matrix],trainable=True)(deep_input)

LSTM_1 = LSTM(512)(embedding_layer)


dense_1 = Dense(2,activation='sigmoid')(LSTM_1)


modelNN = Model(inputs=deep_input,outputs=dense_1)

modelNN.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

modelNN.summary()

history = modelNN.fit(padded_sentences, y, batch_size=32, epochs=300, verbose=1, validation_split=0.2)

modelNN.save(r"D:\\vs code\\python\\DeepLearning\\Projects\\Karen\\v2\\models\\cb2.h5")