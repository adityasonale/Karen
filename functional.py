# -*- coding: utf-8 -*-
"""functional.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TuWhVN_HBk7Tz9W34BnUtlPjCH9Nf81a
"""

import numpy as np
import json
import pandas as pd
import re
import string
import pickle
import random

from sklearn.preprocessing import LabelEncoder

from gensim.models import Word2Vec

from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer


from keras.layers import Embedding,LSTM,Dense,Input
from keras.models import Sequential
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import Adam
from keras.models import Model
from keras.layers import Dropout
from karen_functions_v2.data_loader import Load_tokenizer

"""Dataset Creation for functional responses"""

intents = json.loads(open(r'D:\vs code\python\DeepLearning\Projects\Karen\v2\Datasets\functional\functional.json').read())

data = []

for intent in intents['intents']:
    for pattern in intent['patterns']:
        c = [intent['tag'],pattern]
        data.append(c)

"""Creating a Pandas Dataframe"""

random.shuffle(data)

df = pd.DataFrame(data=data,columns=['tags','patterns'])

df.to_csv(r'D:\vs code\python\DeepLearning\Projects\Karen\v2\Datasets\functional\chatbot_dataset_functionalv2.csv',index=False)

dataset = pd.read_csv(r'D:\vs code\python\DeepLearning\Projects\Karen\v2\Datasets\functional\chatbot_dataset_functionalv2.csv')

dataset.head()

"""Creating a List of Labels"""

labels = list(dataset['tags'].unique())
labels = sorted(labels)

print(labels)

with open(r"D:\vs code\python\DeepLearning\Projects\Karen\v2\Pickles\functional\labels_functional.pickle",'wb') as file:
    pickle.dump(labels,file)
    file.close()

def preprocessing(sentence):
    sentence = sentence.lower()
    sentence = re.sub(r"i'm", "i am", sentence)
    sentence = re.sub(r"he's", "he is", sentence)
    sentence = re.sub(r"she's", "she is", sentence)
    sentence = re.sub(r"it's", "it is", sentence)
    sentence = re.sub(r"that's", "that is", sentence)
    sentence = re.sub(r"what's", "that is", sentence)
    sentence = re.sub(r"where's", "where is", sentence)
    sentence = re.sub(r"how's", "how is", sentence)
    sentence = re.sub(r"\'ll", " will", sentence)
    sentence = re.sub(r"\'ve", " have", sentence)
    sentence = re.sub(r"\'re", " are", sentence)
    sentence = re.sub(r"\'d", " would", sentence)
    sentence = re.sub(r"\'re", " are", sentence)
    sentence = re.sub(r"won't", "will not", sentence)
    sentence = re.sub(r"can't", "cannot", sentence)
    sentence = re.sub(r"n't", " not", sentence)
    sentence = re.sub(r"'til", "until", sentence)
    sentence = re.sub(r"who's", "who is", sentence)
    sentence = re.sub(r"[-()\"#/@;:<>{}`+=~|.\,?]", "", sentence)
    sentence = sentence.split()
    sentence = [lemmatizer.lemmatize(word) for word in sentence]
    sentence = [lemmatizer.lemmatize(word,pos='v') for word in sentence]
    sentence = ' '.join(sentence)

    return sentence

lemmatizer = WordNetLemmatizer()
dataset['patterns'] = dataset['patterns'].apply(lambda x: preprocessing(x))

for i in dataset['patterns']:
    print(i)

df.to_csv(r'D:\vs code\python\DeepLearning\Projects\Karen\v2\Datasets\functional\chatbot_dataset_functional_cleaned.csv',index=False)

encoder = LabelEncoder()

encoder_y = encoder.fit_transform(dataset['tags'])
x = dataset['patterns'].values

y = encoder_y.reshape(-1,1)

for i,j in zip(y,dataset['tags']):
    print(i,j)

a = set()

for sentence in x:
    sentence = sentence.split()
    for word in sentence:
        a.add(word)

len(a)   # the size of the corpus was found to be 284

maxlen = 0

for sentence in x:
    sentence = sentence.split()
    if len(sentence) > maxlen:
        maxlen = len(sentence)

print(maxlen)   # maximum length of the sentence was found to be 8

x = x.tolist()

tokenizer = Tokenizer(num_words=500)
tokenizer.fit_on_texts(x)
# tokenizer = Load_tokenizer()

vocab_size = len(tokenizer.word_index) + 1

x_sequences = tokenizer.texts_to_sequences(x)

padding_length = maxlen + 5

padded_sentences = pad_sequences(x_sequences,maxlen=padding_length)

with open(r"D:\vs code\python\DeepLearning\Projects\Karen\v2\Pickles\functional\padding_length_functional.pickle",'wb') as file:
    pickle.dump(padding_length,file)
    file.close()

x_new = []

for sentence in x:
    sentence = sentence.split()
    x_new.append(sentence)

vector_length = 50

file_path = f"D:\Datasets\glove\glove.6B.{vector_length}d.txt"

embeddings_dictionary = dict()

file = open(file_path,encoding='utf8')

for lines in file:
    records = lines.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary[word] = vector_dimensions

file.close()

embedding_matrix = np.zeros((vocab_size,vector_length))

for word, index in tokenizer.word_index.items():
    vector = embeddings_dictionary.get(word)
    if vector is not None:
        embedding_matrix[index] = vector

with open(r'D:\vs code\python\DeepLearning\Projects\Karen\v2\Pickles\functional\my_dict_functional.pickle', 'wb') as file:
    pickle.dump(embeddings_dictionary, file)
    file.close()

with open(r'D:\vs code\python\DeepLearning\Projects\Karen\v2\Pickles\functional\my_tokenizer_functional.pickle','wb') as file:
    pickle.dump(tokenizer,file)
    file.close()

units = len(dataset['tags'].unique())

deep_input = Input(shape=(maxlen + 5,))

embedding_layer = Embedding(vocab_size,vector_length,weights=[embedding_matrix],trainable=True)(deep_input)

LSTM_1 = LSTM(256)(embedding_layer)


dense_1 = Dense(units,activation='sigmoid')(LSTM_1)


modelNN = Model(inputs=deep_input,outputs=dense_1)

modelNN.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

modelNN.summary()

history = modelNN.fit(padded_sentences, y, batch_size=32, epochs=300, verbose=1, validation_split=0.2)

modelNN.save(r"D:\vs code\python\DeepLearning\Projects\Karen\v2\models\cb2_functional.h5")